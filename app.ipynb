{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20bb9cbea2e474aafefdc0796c635cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad73d800ac64e61aaabbba4176a07db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m df = pd.DataFrame(pages_and_texts)\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m tqdm(pages_and_texts):\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     item[\u001b[33m\"\u001b[39m\u001b[33msentences\u001b[39m\u001b[33m\"\u001b[39m] = [\u001b[38;5;28mstr\u001b[39m(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.sents)]\n\u001b[32m     50\u001b[39m     item[\u001b[33m\"\u001b[39m\u001b[33mpage_sentence_count_spacy\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlen\u001b[39m(item[\u001b[33m\"\u001b[39m\u001b[33msentences\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msplit_list\u001b[39m(input_list: \u001b[38;5;28mlist\u001b[39m, slice_size: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/spacy/language.py:1040\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m   1020\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1021\u001b[39m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1024\u001b[39m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1025\u001b[39m ) -> Doc:\n\u001b[32m   1026\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[33;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[32m   1028\u001b[39m \u001b[33;03m    is preserved.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1038\u001b[39m \u001b[33;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[32m   1039\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1040\u001b[39m     doc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1041\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1042\u001b[39m         component_cfg = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/spacy/language.py:1131\u001b[39m, in \u001b[36mLanguage._ensure_doc\u001b[39m\u001b[34m(self, doc_like)\u001b[39m\n\u001b[32m   1129\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m.vocab).from_bytes(doc_like)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/spacy/language.py:1123\u001b[39m, in \u001b[36mLanguage.make_doc\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m   1119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) > \u001b[38;5;28mself\u001b[39m.max_length:\n\u001b[32m   1120\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1121\u001b[39m         Errors.E088.format(length=\u001b[38;5;28mlen\u001b[39m(text), max_length=\u001b[38;5;28mself\u001b[39m.max_length)\n\u001b[32m   1122\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/spacy/tokenizer.pyx:160\u001b[39m, in \u001b[36mspacy.tokenizer.Tokenizer.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/spacy/tokenizer.pyx:196\u001b[39m, in \u001b[36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/spacy/tokenizer.pyx:400\u001b[39m, in \u001b[36mspacy.tokenizer.Tokenizer._tokenize\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/spacy/tokenizer.pyx:480\u001b[39m, in \u001b[36mspacy.tokenizer.Tokenizer._attach_tokens\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/spacy/vocab.pyx:205\u001b[39m, in \u001b[36mspacy.vocab.Vocab.get\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/spacy/vocab.pyx:234\u001b[39m, in \u001b[36mspacy.vocab.Vocab._new_lexeme\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/spacy/lang/lex_attrs.py:144\u001b[39m, in \u001b[36mlower\u001b[39m\u001b[34m(string)\u001b[39m\n\u001b[32m    140\u001b[39m             shape.append(shape_char)\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(shape)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlower\u001b[39m(string: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m string.lower()\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprefix\u001b[39m(string: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import fitz\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "from spacy.lang.en import English\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "pdf_path = \"human-nutrition-text.pdf\"\n",
    "url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "device = \"cpu\"\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(pdf_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    return text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):\n",
    "        text = text_formatter(page.get_text())\n",
    "        pages_and_texts.append({\n",
    "            \"page_number\": page_number - 41,\n",
    "            \"page_char_count\": len(text),\n",
    "            \"page_word_count\": len(text.split(\" \")),\n",
    "            \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "            \"page_token_count\": len(text) / 4,\n",
    "            \"text\": text\n",
    "        })\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in list(nlp(item[\"text\"]).sents)]\n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n",
    "\n",
    "def split_list(input_list: list, slice_size: int) -> list[list[str]]:\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "num_sentence_chunk_size = 10\n",
    "\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(item[\"sentences\"], num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])\n",
    "\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk)\n",
    "        pages_and_chunks.append({\n",
    "            \"page_number\": item[\"page_number\"],\n",
    "            \"sentence_chunk\": joined_sentence_chunk,\n",
    "            \"chunk_char_count\": len(joined_sentence_chunk),\n",
    "            \"chunk_word_count\": len(joined_sentence_chunk.split(\" \")),\n",
    "            \"chunk_token_count\": len(joined_sentence_chunk) / 4\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(pages_and_chunks)\n",
    "min_token_length = 30\n",
    "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=device)\n",
    "\n",
    "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])\n",
    "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]\n",
    "\n",
    "text_chunks\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Convert text into embeddings\n",
    "embeddings = embedding_model.encode(text_chunks, normalize_embeddings=True)  # Normalize for cosine similarity\n",
    "embedding_dim = embeddings.shape[1]\n",
    "\n",
    "# Create FAISS index for Cosine Similarity\n",
    "index = faiss.IndexFlatIP(embedding_dim)  # Use IndexFlatIP for cosine similarity\n",
    "index.add(embeddings)\n",
    "\n",
    "faiss.write_index(index, \"rag_index.bin\")\n",
    "\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "def retrieve_relevant_resources(query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Retrieves the top-k most relevant resources from FAISS index using cosine similarity.\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return [text_chunks[i] for i in indices[0]]\n",
    "\n",
    "# 2. Setup Hugging Face API LLM\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=device  # Use \"cuda\" for GPU or \"cpu\" for CPU\n",
    ")\n",
    "\n",
    "def prompt_formatter(query: str, \n",
    "                     context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "    # Create a base prompt with examples to help the model\n",
    "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
    "    # We could also write this in a txt file and import it in if we wanted.\n",
    "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "    Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "    Don't return the thinking, only return the answer.\n",
    "    Make sure your answers are as explanatory as possible.\n",
    "    Use the following examples as reference for the ideal answer style.\n",
    "    Include only the answer in the final output.\n",
    "    \\nExample 1:\n",
    "    Query: What are the fat-soluble vitamins?\n",
    "    Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
    "    \\nExample 2:\n",
    "    Query: What are the causes of type 2 diabetes?\n",
    "    Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n",
    "    \\nExample 3:\n",
    "    Query: What is the importance of hydration for physical performance?\n",
    "    Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n",
    "    \\nNow use the following context items to answer the user query:\n",
    "    {context}\n",
    "    \\nRelevant passages: <extract relevant passages from the context here>\n",
    "    User query: {query}\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query   \n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "def rag_generate_response(user_query):\n",
    "    relevant_context = retrieve_relevant_resources(user_query)\n",
    "    context_text = \"\\n\".join(relevant_context)\n",
    "\n",
    "    prompt = prompt_formatter(user_query, [{\"sentence_chunk\": text} for text in context_text])\n",
    "    \n",
    "    outputs = pipe(prompt, max_new_tokens=256)\n",
    "    return outputs[0][\"generated_text\"].strip()\n",
    "\n",
    "# Example Query\n",
    "query = \"What is the Muscular system?\"\n",
    "response = rag_generate_response(query)\n",
    "print(\"\\n[INFO] Assistant Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e302dbe7a1a34997a183ecee2ceae7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aadi/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import fitz\n",
    "import pandas as pd\n",
    "import re\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from spacy.lang.en import English\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# Constants\n",
    "PDF_PATH = \"human-nutrition-text.pdf\"\n",
    "PDF_URL = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "DEVICE = \"cpu\"\n",
    "EMBEDDING_MODEL = \"all-mpnet-base-v2\"\n",
    "MODEL_ID = \"google/gemma-2b-it\"\n",
    "TOP_K = 3\n",
    "\n",
    "# Download PDF if not present\n",
    "if not os.path.exists(PDF_PATH):\n",
    "    response = requests.get(PDF_URL)\n",
    "    if response.status_code == 200:\n",
    "        with open(PDF_PATH, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "# Initialize NLP\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Function to clean text\n",
    "def text_formatter(text: str) -> str:\n",
    "    return text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "# Read and preprocess PDF\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):\n",
    "        text = text_formatter(page.get_text())\n",
    "        pages_and_texts.append({\n",
    "            \"page_number\": page_number,\n",
    "            \"text\": text\n",
    "        })\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(PDF_PATH)\n",
    "\n",
    "# Split text into chunks\n",
    "for item in pages_and_texts:\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in list(nlp(item[\"text\"]).sents)]\n",
    "    item[\"sentence_chunks\"] = [\"\".join(item[\"sentences\"][i:i + 10]).strip() for i in range(0, len(item[\"sentences\"]), 10)]\n",
    "\n",
    "# Flatten chunks into a list\n",
    "pages_and_chunks = [{\"page_number\": item[\"page_number\"], \"sentence_chunk\": chunk} for item in pages_and_texts for chunk in item[\"sentence_chunks\"]]\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL, device=DEVICE)\n",
    "\n",
    "# Compute embeddings\n",
    "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks]\n",
    "embeddings = embedding_model.encode(text_chunks, normalize_embeddings=True)\n",
    "\n",
    "# Store embeddings in FAISS\n",
    "embedding_dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "index.add(embeddings)\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.gemma.modeling_gemma because of the following error (look up to see its traceback):\ncannot import name 'is_flash_attn_greater_or_equal' from 'transformers.utils' (/Users/aadi/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1390\u001b[39m, in \u001b[36m_get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1383\u001b[39m FAISS_IMPORT_ERROR = \"\"\"\n\u001b[32m   1384\u001b[39m {0} requires the faiss library but it was not found in your environment. Checkout the instructions on the\n\u001b[32m   1385\u001b[39m installation page of its repo: https://github.com/facebookresearch/faiss/blob/master/INSTALL.md and follow the ones\n\u001b[32m   1386\u001b[39m that match your environment. Please note that you may need to restart your runtime after installation.\n\u001b[32m   1387\u001b[39m \"\"\"\n\u001b[32m-> \u001b[39m\u001b[32m1390\u001b[39m # docstyle-ignore\n\u001b[32m   1391\u001b[39m PYTORCH_IMPORT_ERROR = \"\"\"\n\u001b[32m   1392\u001b[39m {0} requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\n\u001b[32m   1393\u001b[39m installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n\u001b[32m   1394\u001b[39m Please note that you may need to restart your runtime after installation.\n\u001b[32m   1395\u001b[39m \"\"\"\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:995\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/transformers/models/gemma/modeling_gemma.py:31\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_attn_mask_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AttentionMaskConverter\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_flash_attention_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FlashAttentionKwargs\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     33\u001b[39m     BaseModelOutputWithPast,\n\u001b[32m     34\u001b[39m     CausalLMOutputWithPast,\n\u001b[32m     35\u001b[39m     SequenceClassifierOutputWithPast,\n\u001b[32m     36\u001b[39m     TokenClassifierOutput,\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/transformers/modeling_flash_attention_utils.py:23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_flash_attn_2_available, is_flash_attn_greater_or_equal, logging\n\u001b[32m     26\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'is_flash_attn_greater_or_equal' from 'transformers.utils' (/Users/aadi/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load tokenizer and LLM API\u001b[39;00m\n\u001b[32m      2\u001b[39m tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m pipe = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorch_dtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Retrieve relevant context from FAISS\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve_relevant_resources\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m, top_k: \u001b[38;5;28mint\u001b[39m = TOP_K):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:905\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m    900\u001b[39m model, default_revision = get_default_model_and_revision(targeted_task, framework, task_options)\n\u001b[32m    901\u001b[39m revision = revision \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m default_revision\n\u001b[32m    902\u001b[39m logger.warning(\n\u001b[32m    903\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo model was supplied, defaulted to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and revision\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mUsing a pipeline without specifying a model name and revision in production is not recommended.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    906\u001b[39m )\n\u001b[32m    907\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33mrevision\u001b[39m\u001b[33m\"\u001b[39m] = revision\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:250\u001b[39m, in \u001b[36minfer_framework_load_model\u001b[39m\u001b[34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[39m\n\u001b[32m    248\u001b[39m class_tuple = ()\n\u001b[32m    249\u001b[39m look_pt = is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m framework \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m look_tf = is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m framework \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_classes:\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m look_pt:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1381\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1380\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1392\u001b[39m, in \u001b[36m_get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1383\u001b[39m FAISS_IMPORT_ERROR = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m   1384\u001b[39m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m requires the faiss library but it was not found in your environment. Checkout the instructions on the\u001b[39m\n\u001b[32m   1385\u001b[39m \u001b[33minstallation page of its repo: https://github.com/facebookresearch/faiss/blob/master/INSTALL.md and follow the ones\u001b[39m\n\u001b[32m   1386\u001b[39m \u001b[33mthat match your environment. Please note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[32m   1387\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m   1390\u001b[39m \u001b[38;5;66;03m# docstyle-ignore\u001b[39;00m\n\u001b[32m   1391\u001b[39m PYTORCH_IMPORT_ERROR = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\u001b[39m\n\u001b[32m   1393\u001b[39m \u001b[33minstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\u001b[39m\n\u001b[32m   1394\u001b[39m \u001b[33mPlease note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[32m   1395\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m   1398\u001b[39m \u001b[38;5;66;03m# docstyle-ignore\u001b[39;00m\n\u001b[32m   1399\u001b[39m TORCHVISION_IMPORT_ERROR = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m   1400\u001b[39m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m requires the Torchvision library but it was not found in your environment. Checkout the instructions on the\u001b[39m\n\u001b[32m   1401\u001b[39m \u001b[33minstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\u001b[39m\n\u001b[32m   1402\u001b[39m \u001b[33mPlease note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[32m   1403\u001b[39m \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.models.gemma.modeling_gemma because of the following error (look up to see its traceback):\ncannot import name 'is_flash_attn_greater_or_equal' from 'transformers.utils' (/Users/aadi/Desktop/LLM/RAG/rag-app/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and LLM API\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "pipe = pipeline(\"text-generation\", model=MODEL_ID, model_kwargs={\"torch_dtype\": torch.bfloat16}, device=DEVICE)\n",
    "\n",
    "# Retrieve relevant context from FAISS\n",
    "def retrieve_relevant_resources(query: str, top_k: int = TOP_K):\n",
    "    query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return [text_chunks[i] for i in indices[0]]\n",
    "\n",
    "# Generate response\n",
    "def rag_generate_response(user_query):\n",
    "    relevant_context = retrieve_relevant_resources(user_query)\n",
    "    print(relevant_context)\n",
    "    #prompt = \n",
    "    #inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    #outputs = pipe(inputs, max_new_tokens=256)\n",
    "    #return outputs[0][\"generated_text\"].strip()\n",
    "\n",
    "# Example Query\n",
    "#query = \"What is the Muscular system?\"\n",
    "#response = rag_generate_response(query)\n",
    "#print(\"\\n[INFO] Assistant Response:\")\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rag_generate_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrag_generate_response\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'rag_generate_response' is not defined"
     ]
    }
   ],
   "source": [
    "rag_generate_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
